{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActivationFunctions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mainak555/lb-ai/blob/ai/ActivationFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jmwWKxRUXkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "class Squash:\n",
        "    def __init__(self):\n",
        "        self.function = {\n",
        "            'sigmoid': lambda x: 1 / (1 + np.exp(-x)),\n",
        "            'tanh': lambda x: np.tanh(x),\n",
        "            'arctan': lambda x: np.arctan(x),\n",
        "            'softmax': lambda x: (lambda exps= np.exp(x): exps / float(sum(exps)))(),\n",
        "            'softplus': lambda x: np.log(1 + np.exp(x)),\n",
        "            'swish': lambda x: x / (1 + np.exp(-x)),\n",
        "            'identity': lambda x: x,\n",
        "            'relu': lambda x: [item if item >= 0 else 0 for item in x],\n",
        "            'prelu': lambda x, alpha: [item if item >= 0 else alpha * item for item in x],\n",
        "            'elu': lambda x, alpha: [item if item >= 0 else np.dot(alpha, np.exp(item) - 1) for item in x]\n",
        "            }            \n",
        "        \n",
        "        self.derivative = {\n",
        "            'sigmoid': lambda x: (lambda fx= self.function['sigmoid'](x): fx * (1 - fx))(),\n",
        "            'tanh': lambda x: (lambda fx = self.function['tanh'](x): 1 - fx**2)(),\n",
        "            'arctan': lambda x: 1 / (1 + x**2),\n",
        "            'softmax': lambda x: (lambda fx= self.function['softmax'](x): fx * (1 - fx))(),\n",
        "            'softplus': lambda x: self.function['sigmoid'](x),\n",
        "            'swish': lambda x: (lambda x= x, fx= x * self.function['sigmoid'](x): fx + (1 - fx) * self.function['sigmoid'](x))(),\n",
        "            'identity': lambda x: np.ones(x.shape[0]),\n",
        "            'relu': lambda x: [1 if item >= 0 else 0 for item in x],\n",
        "            'prelu': lambda x, alpha: [1 if item >= 0 else alpha for item in x],\n",
        "            'elu': lambda x, alpha: [1 if item >= 0 else sum(self.function['elu']([item], alpha), alpha) for item in x]\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}